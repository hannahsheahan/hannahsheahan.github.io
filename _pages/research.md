---
permalink: /
title: ""
#excerpt: "About me"
author_profile: true
redirect_from:
  - /research/
  - /research.html
---

I am currently a Postdoc at the University of Oxford. In June 2021 I will be starting as a Research Engineer at [DeepMind](http://deepmind.com) in London. I am interested in embodied learning, generalisation and the effects of context and task structure on learning in humans and neural network models.

On the side, I'm also a Research Associate of [Wadham College](https://www.wadham.ox.ac.uk/), where I organise [talks](https://www.wadham.ox.ac.uk/about-wadham/wadhams-people/research-associates) on topics I care about like climate change, Oxford's uncomfortable colonial past, and the intersection of psychology and AI. I did my PhD at the [Computational and Biological Learning Lab](http://learning.eng.cam.ac.uk/Public/) at the University of Cambridge. Prior to that I did an undergraduate degree in engineering.


Research
------

### Learning relational structure
Psychologists use the term 'structure learning' to describe how humans learn invariances over relational patterns in data. For example, the concept we call *magnitude* describes a structure that relates stimuli to each other along a single dimension. Abstracting the relationships between stimuli across contexts permits new inferences, such as knowing that both cheetahs and space rockets move 'fast', even though animals and vehicles belong in different semantic categories, do not look alike and move at different speeds. How do humans and neural networks learn abstract knowledge of simple relational structures like magnitude? How does action impact or afford the learning of spatial or more abstract relations between things? I've been trying to answer some of these questions with [Chris Summerfield](https://www.psy.ox.ac.uk/team/christopher-summerfield) at Oxford University. Here is a short 3 minute talk from the 2021 Oxford Neuroscience Symposium where I talk about some of this work.

<iframe width="560" height="315" src="https://www.youtube.com/embed/c6FNHgxBbYI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Learning theory
What principles underlie learning in neural network models and how can we understand them? What makes some things easy to learn and other things hard? Can we predict learning dynamics? Can these tools enable us to predict the geometry of neural representations in animals performing different tasks?

### Motor learning & control
How do we humans learn and adapt our movements as we experience the world? And how do we learn and parcellate lots of different motor skills, without overwriting others? Here's [a cute video](https://zuckermaninstitute.columbia.edu/brain-science-baseball) illustrating some of my PhD work on this topic with Daniel Wolpert (then at Cambridge University).

<iframe width="560" height="315" src="https://www.youtube.com/embed/QWaUyTiukKI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
